{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCI solution\n",
    "\n",
    "train.csvは重いのでupしてないです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nmiuchi\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "test = pd.read_csv(\"../data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UnderSamplingに用いるモデルの数\n",
    "us_num  =15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Encoding用のクラスの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetEncoding:\n",
    "    def __init__(self, col_names, target):\n",
    "        self.col_names = col_names\n",
    "        self.target = target\n",
    "\n",
    "    \"\"\"trainデータ用\"\"\"\n",
    "    def mean_train_encoding(self, df):\n",
    "        y_tr = df[self.target].values\n",
    "        skf = StratifiedKFold(5, shuffle = True, random_state=0)\n",
    "        # TE後の名前を用意する.\n",
    "        for col in self.col_names:\n",
    "            df[col + '_mean_encoded'] = np.nan\n",
    "\n",
    "        # trn : training, val : validation\n",
    "        for trn_ind , val_ind in skf.split(df, y_tr):\n",
    "            x_tr, x_val = df.iloc[trn_ind], df.iloc[val_ind]\n",
    "            for col in self.col_names:\n",
    "                tr_mean = x_tr.groupby(col)[self.target].mean()\n",
    "                mean = x_val[col].map(tr_mean)\n",
    "                df[col + '_mean_encoded'].iloc[val_ind] = mean\n",
    "\n",
    "        prior = df[self.target].mean()\n",
    "        for col in self.col_names:\n",
    "            df[col + '_mean_encoded'].fillna(prior, inplace = True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    \"\"\"\"testデータ用\"\"\"\n",
    "    def mean_test_encoding(self, df_trn, df_tst):\n",
    "        # TE後の名前を用意する.\n",
    "        for col in self.col_names:\n",
    "            df_tst[col + '_mean_encoded'] = np.nan\n",
    "\n",
    "        # encoding用の前処理\n",
    "        for col in self.col_names:\n",
    "            tr_mean = df_trn.groupby(col)[self.target].mean()\n",
    "            mean = df_tst[col].map(tr_mean)\n",
    "            df_tst[col + '_mean_encoded'] = mean\n",
    "\n",
    "        prior = df_trn[self.target].mean()\n",
    "        # testにはあるがtrainにはないカテゴリに対して平均値を入れる.\n",
    "        for col in self.col_names:\n",
    "            df_tst[col + '_mean_encoded'].fillna(prior, inplace = True)\n",
    "\n",
    "        return df_tst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用するカテゴリカル変数の用意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['state', 'stop_date', 'stop_time', 'location_raw', 'county_name',\n",
      "       'fine_grained_location', 'police_department', 'driver_gender',\n",
      "       'driver_race_raw', 'driver_race', 'violation_raw', 'violation',\n",
      "       'search_type_raw', 'search_type', 'officer_id', 'stop_duration'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# カテゴリカル変数の確認\n",
    "print(train.select_dtypes([\"object\"]).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カテゴリカル変数で同じカテゴリに対してrawとrawじゃないもの両方があればrawを残したカラムのリスト\n",
    "# stateは中身がすべて同じなので取り除いた。\n",
    "columns_name_raw = ['stop_date', \n",
    "                    'stop_time', \n",
    "                    'location_raw', \n",
    "                    'county_name',\n",
    "                    'fine_grained_location', \n",
    "                    'police_department', \n",
    "                    'driver_gender',\n",
    "                    'driver_race_raw', \n",
    "                    'violation_raw',\n",
    "                    'search_type_raw', \n",
    "                    'officer_id', \n",
    "                    'stop_duration']\n",
    "\n",
    "columns_name_raw_list = list(itertools.combinations(columns_name_raw, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Encoding(CE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainとtestの結合\n",
    "trts_data = pd.concat([train, test], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 66/66 [00:08<00:00,  7.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# 文字列としてカラム同士を結合したものを新たな記述子とする。\n",
    "str_collocation_trts_dct = {\"for_count\":list(np.ones(len(trts_data)))}\n",
    "col_drop_list = []\n",
    "\n",
    "for col_comb in tqdm(columns_name_raw_list):\n",
    "    str_collocation_trts_dct[str(col_comb[0]) + \"_\" + str(col_comb[1])] = trts_data[col_comb[0]].astype(\"str\") + trts_data[col_comb[1]].astype(\"str\")\n",
    "    col_drop_list.append(str(col_comb[0]) + \"_\" + str(col_comb[1]))\n",
    "    \n",
    "str_collocation_trts_df = pd.DataFrame(str_collocation_trts_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 66/66 [02:38<00:00,  2.39s/it]\n"
     ]
    }
   ],
   "source": [
    "# Count Encoding(CE)\n",
    "for_groupby_names = str_collocation_trts_df.columns.drop([\"for_count\"])\n",
    "CE_dct = {}\n",
    "\n",
    "for i in tqdm(for_groupby_names):\n",
    "    CE_dct[i + \"_count\"] = str_collocation_trts_df[i].map(str_collocation_trts_df.groupby(i).count()[\"for_count\"])\n",
    "    \n",
    "trts_count_df = pd.DataFrame(CE_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainとtestの分割\n",
    "train_count_df = trts_count_df.iloc[:len(train), :]\n",
    "test_count_df = trts_count_df.iloc[len(train):, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Encoding(TE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nmiuchi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\nmiuchi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# TEのための処理\n",
    "TE_train_df = str_collocation_trts_df.iloc[:len(train), :]\n",
    "TE_test_df = str_collocation_trts_df.iloc[len(train):, :]\n",
    "\n",
    "TE_train_df[\"is_arrested\"] = train.loc[:, [\"is_arrested\"]]\n",
    "TE_test_df[\"is_arrested\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nmiuchi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\nmiuchi\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "C:\\Users\\nmiuchi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\nmiuchi\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5430: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "C:\\Users\\nmiuchi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\nmiuchi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Target Encoding\n",
    "TE_train_arrest = TargetEncoding(TE_train_df.columns, \"is_arrested\")\n",
    "TE_train = TE_train_arrest.mean_train_encoding(TE_train_df)\n",
    "TE_test = TE_train_arrest.mean_test_encoding(TE_train_df, TE_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# カラムにarrestedや元のカテゴリカル変数が含まれたままなので削除する。\n",
    "col_drop_list.extend([\"is_arrested\", \"is_arrested_mean_encoded\", \"for_count\", \"for_count_mean_encoded\"])\n",
    "col_name_comb_list = TE_test.columns.drop(col_drop_list)\n",
    "\n",
    "TE_train = TE_train.loc[:, col_name_comb_list]\n",
    "TE_test = TE_test.loc[:, col_name_comb_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEとCEの結合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_train = pd.concat([TE_train, train_count_df], axis=1)\n",
    "concat_test = pd.concat([TE_test, test_count_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:01<00:00, 12.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# UnderSampling\n",
    "y_train = train.loc[:, \"is_arrested\"]\n",
    "y_arrested = y_train[y_train==1]\n",
    "y_us_list =[]\n",
    "X_us_list =[]\n",
    "\n",
    "for i in tqdm(range(us_num)):\n",
    "    y_Notarrested = y_train[y_train!=1]\n",
    "    \n",
    "    # y_arrestedと同じ数だけのデータをランダムに取ってくる。\n",
    "    y_Notarrested_RandomSampled = y_Notarrested.reindex(np.random.permutation(y_Notarrested.index)).iloc[:len(y_arrested)]\n",
    "    y_concatenated = pd.concat([y_arrested, y_Notarrested_RandomSampled])\n",
    "    \n",
    "    # リストに格納\n",
    "    y_us_list.append(y_concatenated)\n",
    "    X_us_list.append(concat_train.loc[y_concatenated.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:37,  2.49s/it]\n"
     ]
    }
   ],
   "source": [
    "# 予測\n",
    "result_prob_dct = {}\n",
    "counter = 0\n",
    "for i, j in tqdm(zip(X_us_list, y_us_list)):\n",
    "    counter += 1\n",
    "    # モデルの用意\n",
    "    clf = LGBMClassifier(n_jobs=5)\n",
    "    clf.fit(i, j)\n",
    "    result_prob_dct[\"LGBM_model_\" + str(counter)] = clf.predict_proba(concat_test).T[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 平均値をとる.\n",
    "y_pred = pd.DataFrame(result_prob_dct).mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル精度確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LGBMClassifier()\n",
    "kf = KFold(n_splits=5, random_state=0, shuffle=True)\n",
    "pred_cv = cross_val_predict(clf, concat_train, train[\"is_arrested\"], cv = kf, method='predict_proba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9082975242113124"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(train[\"is_arrested\"], pred_cv[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
